# Datawhale AI 冬令营

## 知识储备

* 微调的流程

  > 图片转自 `Datawhale`

  ![想达成优质效果的“微调工程”](https://my-note-drawing-bed-1322822796.cos.ap-shanghai.myqcloud.com/picture/586f90aa-7ace-40be-8c33-fe95aced7847.png)

  * 准备阶段

    * 数据集收集

      * 如何收集
        * 通过搜索引擎搜索 `剧本`
        * 爬取文章评论与视频分析
        * OCR 技术

    * 数据处理

      > 需要转为纯文本

      * 文本格式

        * `JSON` 与 `JSONL` ：区别是 `JSONL` 一行就是一个 `JSON` 对象
        * `JSON` / `JSONL` 中数据集格式
          * `AIpace`
          * `ShareGPT`

      * 构建数据集思路

        > 角色扮演对话类

        * 提取对话和对话
          * 形成 QA 问答
          * 工具：[KMnO4-zx/extract-dialogue: 从小说中提取对话数据集](https://github.com/KMnO4-zx/extract-dialogue)
        * 筛选需要角色的对话
        * 对话换为我们需要的格式
          * 整理成 `JSONL` 格式的数据

        ---

        > 文本风格输出类

        * 大文本切分（作为 `output`）
          * `py` 程序处理
        * 用大模型批量处理，总结（作为 `input`）
          * [星火大模型精调平台-批量操作](https://training.xfyun.cn/batchInference)
        * 编写 `py` 输出为 `JSONL`文件

  * 训练与调优

    * 微调参数

      * **深度学习** 参数

        * 学习率

          * 寻找最低点
            * 学习率大：大步走，可能跳过最低点
            * 学习率小：小步走，精确但耗时

        * 训练次数

          * epoch（完整遍历一次训练集的所有数据）的次数

            * epoch 少会欠拟合

            * epoch 多可能会过拟合

              > 想象你反复走遍山谷：
              >
              > - **少 Epoch**：可能还没探索到最低点。
              > - **多 Epoch**：可能能找到最低点，但继续下去可能徒劳无功。

        * 输入序列分词后最大程度

      * **`LoRA`** 参数

      * **加速训练** 参数

## 实操记录

## 参考链接

* [Datawhale-微调玩法攻略](https://www.datawhale.cn/activity/110/21/82?rankingPage=1)